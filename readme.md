ğŸ¯ Goal

A single NumPy wrapper that:

Runs on top of the latest stable NumPy (â‰¥2.0).

Provides backward compatibility for older NumPy APIs (1.x, 1.21, etc.).

Optionally swaps backends (CuPy, Torch, JAX) without code changes.

Minimizes breakage in AI/ML libraries that hard-pin weird NumPy versions.

ğŸ› ï¸ Core Components
1. Base Layer (NumPy â‰¥2.x)

Import the newest NumPy internally:

import numpy as _np


Expose everything by default:

from numpy import *


This ensures modern code works natively.

2. Legacy Compatibility Layer

A shim that re-creates removed aliases and functions:

Aliases:

_np.int = int
_np.float = float
_np.bool = bool


Functions:

def asscalar(a): return a.item()
def matrix(data, *args, **kwargs): return _np.array(data, *args, **kwargs)


Catch-all for missing attributes:

class OmniNP:
    def __getattr__(self, name):
        if name in ("int", "float", "bool"):
            return getattr(_np, "int64")
        if name == "asscalar":
            return lambda a: a.item()
        raise AttributeError(name)
np = OmniNP()

3. Backend Abstraction Layer

A toggle system:

BACKEND = "numpy"

def set_backend(name):
    global BACKEND
    if name in ("numpy", "torch", "cupy", "jax"):
        BACKEND = name
    else:
        raise ValueError("Unsupported backend")


Wrapper functions:

def array(x, *args, **kwargs):
    if BACKEND == "torch":
        import torch
        return torch.tensor(x, *args, **kwargs)
    if BACKEND == "cupy":
        import cupy
        return cupy.array(x, *args, **kwargs)
    return _np.array(x, *args, **kwargs)

4. Version Emulation Profiles

Profiles like "1.21", "1.19", "2.x".

Each profile enables/patches features:

def emulate(version="1.21"):
    if version.startswith("1."):
        _np.int = int
        _np.float = float
        _np.bool = bool
        _np.asscalar = lambda a: a.item()

5. Testing Matrix

Run CI against:

NumPy 1.19, 1.21, 2.x

Libraries: TensorFlow, PyTorch, Pandas

Verify import omninumpy as np doesnâ€™t break.

6. Optional Extensions

Auto-detect when a library is breaking and enable the right profile.

Config file (~/.omninumpy.json) for defaults.

Logging/warnings when old APIs are used.

ğŸš€ Steps to Build

Scaffold repo â†’ omninumpy/ with __init__.py.

Base import â†’ wrap NumPy â‰¥2.x.

Legacy shim â†’ implement aliases + removed functions.

Backend toggle â†’ add set_backend().

Profiles â†’ add emulate("1.21").

Testing â†’ scripts that run old + new libraries.

Release â†’ package on PyPI as omninumpy.


Hereâ€™s a **visual-style architecture overview** for **Omninumpy (or Omninp)** to show how weâ€™d structure it:

---

# ğŸŒ Omninumpy High-Level Architecture

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚                 User Code                     â”‚
 â”‚   import omninumpy as np                      â”‚
 â”‚                                               â”‚
 â”‚   # Works across old + new versions           â”‚
 â”‚   np.int, np.asscalar, np.array([...])        â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚         Omninumpy Compatibility Layer         â”‚
 â”‚                                               â”‚
 â”‚  - Re-add deprecated APIs                     â”‚
 â”‚    (np.int, np.float, np.bool, asscalar)      â”‚
 â”‚  - Shims for 1.x â†’ 2.x changes                â”‚
 â”‚  - Error catching & redirecting               â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚          Backend Abstraction Layer            â”‚
 â”‚                                               â”‚
 â”‚  set_backend("numpy" | "cupy" | "torch" | "jax")  â”‚
 â”‚                                               â”‚
 â”‚  array(), dot(), matmul() wrappers            â”‚
 â”‚  Dispatches to correct backend implementation â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚           Version Emulation Profiles          â”‚
 â”‚                                               â”‚
 â”‚  emulate("1.19") â†’ enable legacy functions    â”‚
 â”‚  emulate("1.21") â†’ align dtype behaviors      â”‚
 â”‚  emulate("2.x")  â†’ strict new NumPy rules     â”‚
 â”‚                                               â”‚
 â”‚  Config-driven: ~/.omninp.json or ENV vars    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚            Base NumPy â‰¥ 2.x Core              â”‚
 â”‚                                               â”‚
 â”‚  Always imports latest NumPy (â‰¥2.x)           â”‚
 â”‚  Uses as foundation for universal API         â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ Key Ideas

1. **Top-down flow**:
   User calls `omninumpy` â†’ hits **compat layer** â†’ passes through backend toggle â†’ applies profile rules â†’ executes on NumPy â‰¥2.x (or another backend).

2. **Legacy support**:

   * Shims for **1.x removed functions/aliases**.
   * Profiles simulate older behaviors (like `matrix`, `asscalar`, or dtype coercions).

3. **Future-proofing**:

   * Base always tracks **latest NumPy release**.
   * Profiles + shims handle breakage automatically.

4. **Extendable**:

   * Can plug in JAX, Torch, CuPy just by implementing the same wrapper API.

---

âš¡ This way, you end version hell:

* Old libraries written for NumPy 1.19 â†’ still work.
* New ML frameworks requiring NumPy 2.x â†’ also work.
* Users can flip backends (CPU/GPU) without changing code.

---
Do you want me to also sketch out **the first-pass API design** (what functions we implement in v1)? That way we can start small and grow.

Mapped out a full API design: import stays dead simple, legacy shims live behind `emulate()`, backend switching works with `set_backend()`, and we can optionally wire in WebSockets for remote execution if you want to get spicy.


# ğŸ”Œ Omninumpy API Design

Hereâ€™s a breakdown of how the API should look and behave, including whether WebSockets make sense.

---

## ğŸ¯ Core Import Behavior

Users always import the same way:

```python
import omninumpy as np
```

Thatâ€™s it. Omninumpy decides under the hood whether it needs to shim old APIs, switch backends, or run in strict NumPy â‰¥2.x mode.

---

## ğŸ› ï¸ API Layers

### 1. Base NumPy API

* Exposes **all current NumPy functions** directly.
* Behaves like modern NumPy (â‰¥2.0).

Example:

```python
np.array([1, 2, 3])
np.dot(a, b)
```

---

### 2. Legacy Compatibility

* Provides shims for **removed/renamed functions**.
* Behaves like older NumPy when `emulate("1.x")` is called.

Example:

```python
np.int, np.float, np.bool    # Restored
np.asscalar(x)               # Works again
np.matrix([[1,2],[3,4]])     # Wraps np.array()
```

---

### 3. Backend Abstraction

Users can change computation backend with a single call:

```python
np.set_backend("torch")
np.set_backend("cupy")
np.set_backend("jax")
```

Example:

```python
x = np.array([1, 2, 3])   # torch.tensor if backend=torch
```

Wrapper functions mirror NumPy's API for implemented functions:

```python
np.array, np.dot, np.matmul, np.linalg.inv  # These are backend-aware
# Other functions fall back to NumPy (see limitations below)
```

---

### 4. Version Emulation

Switch profiles to restore old semantics:

```python
np.emulate("1.19")
np.emulate("1.21")
np.emulate("2.x")
```

This changes:

* dtype aliasing
* deprecated function support
* matrix/asscalar availability

---

## âš™ï¸ Configuration

* Defaults can be stored in `~/.omninumpy.json`:

```json
{
  "backend": "jax",
  "device": "gpu",
  "jit": true,
  "jit_threshold": 1000,
  "profile": "1.21",
  "auto_select_backend": false
}
```

* Environment variables override config:

```bash
OMNINP_BACKEND=jax:gpu
OMNINP_JIT=true
OMNINP_JIT_THRESHOLD=1000
OMNINP_PROFILE=1.19
OMNINP_AUTO_BACKEND=true
```

## ğŸš€ Installation

```bash
pip install omninumpy
```

Optional backends:
```bash
pip install omninumpy[torch]  # For PyTorch backend
pip install omninumpy[cupy]  # For CuPy backend
pip install omninumpy[jax]   # For JAX backend
```

## ğŸ“– Usage

### Basic Usage

```python
import omninumpy as np

# Works like regular NumPy
a = np.array([1, 2, 3])
b = np.dot(a, a)
```

### Legacy Compatibility

```python
import omninumpy as np

# Enable legacy APIs
np.emulate("1.21")

# Now old APIs work
print(np.int)  # <class 'numpy.int64'>
scalar = np.asscalar(np.array([42]))  # 42
```

### Backend Switching

```python
import omninumpy as np

# Switch to PyTorch
np.set_backend("torch")
a = np.array([1, 2, 3])  # Creates torch.Tensor

# Switch to CuPy
np.set_backend("cupy")
b = np.array([4, 5, 6])  # Creates cupy.ndarray

# Advanced JAX with device and JIT
np.set_backend("jax:gpu")  # GPU acceleration
c = np.array([7, 8, 9])    # JAX array on GPU
d = np.dot(c, c)           # JIT compiled for speed
```

### JAX Advanced Features

```python
import omninumpy as np

# Device-aware backends
np.set_backend("jax:cpu")   # CPU only
np.set_backend("jax:gpu")   # GPU acceleration
np.set_backend("jax:tpu")   # TPU acceleration

# JIT compilation with smart thresholds
# Large arrays automatically use JIT for performance
a = np.random.random((2000, 2000))  # Large array â†’ JIT compiled
b = np.random.random((100, 100))    # Small array â†’ eager execution
c = np.dot(a, b)  # Fast JIT execution

# Interoperability
numpy_array = np.to_numpy(jax_array)    # JAX â†’ NumPy
jax_array = np.to_backend(numpy_array)  # NumPy â†’ JAX (on current device)
```

### Auto Backend Selection

```python
import omninumpy as np

# Automatically pick fastest available backend
backend = np.auto_backend()
np.set_backend(backend)
```

Or via config:

```json
{
  "auto_select_backend": true
}
```

### Performance Monitoring

```python
import omninumpy as np

# Use functions
a = np.array([1, 2, 3])

# Get timing data
timings = np.get_timings()
print(timings)  # {'_array': 0.000123}
```

### Validation

```python
import omninumpy as np

a = np.array([1, 2, 3])
np.validate_array(a, shape=(3,), dtype=np.int64)
```

## ğŸ”„ Migration Guide

### From NumPy 1.x

If your code uses deprecated NumPy 1.x APIs:

1. Install omninumpy
2. Change `import numpy as np` to `import omninumpy as np`
3. Add `np.emulate("1.21")` at the top of your script
4. Your code should work unchanged

### Backend Migration

To use GPU acceleration without code changes:

1. Install backend: `pip install omninumpy[torch]`
2. Add `np.set_backend("torch")` before computations
3. Arrays automatically become torch tensors

### Configuration

Create `~/.omninumpy.json`:

```json
{
  "backend": "torch",
  "profile": "1.21",
  "auto_select_backend": false
}
```

This sets defaults for all imports.

---

## ğŸ“¡ WebSocket Integration (Optional)

### Why?

* Could allow **remote backend execution** (e.g. run arrays on a GPU server while client runs lightweight code).
* Lets multiple processes share one backend session.

### How?

* Provide `set_backend("ws://server:port")`.
* Under the hood, Omninumpy serializes arrays (using pickle or Arrow), sends over WebSocket.
* Remote worker (server) executes NumPy/Torch/CuPy and returns result.

Example:

```python
np.set_backend("ws://localhost:9000")
np.array([1, 2, 3])   # Executed remotely
```

### API Impact

* Transparent to user.
* Extra dependency: `websockets` or `socket.io`.
* Useful for distributed/remote compute scenarios.

---

## ğŸš€ API Summary

* **Core:** Works like modern NumPy.
* **Compat:** Old APIs restored if needed.
* **Backend toggle:** Local CPU/GPU or Torch/JAX.
* **Profiles:** Emulate specific versions.
* **Optional WebSocket:** Offload execution to remote workers.

This keeps the surface familiar (NumPy-like) while hiding all the messy backend switching and compatibility logic.

## âœ… Implemented Features

### Backend-Aware Functions
| Function | NumPy | PyTorch | CuPy | JAX |
|----------|-------|---------|------|-----|
| `array` | âœ… | âœ… | âœ… | âœ… |
| `dot` | âœ… | âœ… | âœ… | âœ… (JIT) |
| `matmul` | âœ… | âœ… | âœ… | âœ… (JIT) |
| `mean` | âœ… | âœ… | âœ… | âœ… (JIT) |
| `sum` | âœ… | âœ… | âœ… | âœ… (JIT) |
| `stack` | âœ… | âœ… | âœ… | âœ… |
| `concatenate` | âœ… | âœ… | âœ… | âœ… |
| `eye` | âœ… | âœ… | âœ… | âœ… |
| `zeros` | âœ… | âœ… | âœ… | âœ… |
| `ones` | âœ… | âœ… | âœ… | âœ… |
| `reshape` | âœ… | âœ… | âœ… | âœ… (JIT) |
| `transpose` | âœ… | âœ… | âœ… | âœ… (JIT) |
| `astype` | âœ… | âœ… | âœ… | âœ… |
| `clip` | âœ… | âœ… | âœ… | âœ… (JIT) |
| `where` | âœ… | âœ… | âœ… | âœ… |

### Linear Algebra Functions
| Function | NumPy | PyTorch | CuPy | JAX |
|----------|-------|---------|------|-----|
| `linalg.inv` | âœ… | âœ… | âœ… | âœ… (JIT) |
| `linalg.svd` | âœ… | âœ… | âœ… | âœ… (JIT) |
| `linalg.eig` | âœ… | âœ… | âœ… | âœ… (JIT) |
| `linalg.cholesky` | âœ… | âœ… | âœ… | âœ… (JIT) |
| `linalg.qr` | âœ… | âœ… | âœ… | âœ… (JIT) |

## âš ï¸ Current Limitations

- **Partial Coverage**: ~16 core functions are backend-aware. All others fall back to NumPy or raise `AttributeError` in non-NumPy backends.
- **JAX JIT**: Only applied to performance-critical functions (matrix ops, reductions). Other operations use eager execution.
- **Device Support**: JAX supports CPU/GPU/TPU placement. PyTorch/CuPy use their native device management.
- **Error Handling**: Unimplemented functions raise clear errors instead of silent NumPy fallback.
- **Testing**: Comprehensive backend-specific tests ensure type correctness and numerical accuracy.
- **Benchmarks**: Use backend-native random generation for fair performance comparison.

## ğŸš§ Roadmap

- Complete backend wrapping for all common NumPy functions
- Add strict backend isolation with error raising
- Expand linear algebra coverage
- Add comprehensive cross-backend tests
- Optimize backend switching performance
